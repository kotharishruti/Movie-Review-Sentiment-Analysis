{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn import naive_bayes, svm, preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing movie reviews\n",
    "def clean_review(raw_review):\n",
    "    # Remove HTML markup\n",
    "    text = BeautifulSoup(raw_review, \"lxml\")\n",
    "    \n",
    "    #Removing digits and punctuation\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.get_text())\n",
    "    \n",
    "    #Converting to lowercase\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in text if w not in stops]\n",
    "    \n",
    "    # Return a cleaned string\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a feature vector(word2vec averaging) for each movie review\n",
    "def review_to_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    This function generates a feature vector for the given review.\n",
    "    Input:\n",
    "        words: a list of words extracted from a review\n",
    "        model: trained word2vec model\n",
    "        num_features: dimension of word2vec vectors\n",
    "    Output:\n",
    "        a numpy array representing the review\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vec = np.zeros((num_features), dtype=\"float32\")\n",
    "    word_count = 0\n",
    "    \n",
    "    # index2word_set is a set consisting of all words in the vocabulary\n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            word_count += 1\n",
    "            feature_vec += model[word]\n",
    "\n",
    "    feature_vec /= word_count\n",
    "    return feature_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates vectorized movie reviews\n",
    "def gen_review_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Function which generates a m-by-n numpy array from all reviews,\n",
    "    where m is len(reviews), and n is num_feature\n",
    "    Input:\n",
    "            reviews: a list of lists. \n",
    "                     Inner lists are words from each review.\n",
    "                     Outer lists consist of all reviews\n",
    "            model: trained word2vec model\n",
    "            num_feature: dimension of word2vec vectors\n",
    "    Output: m-by-n numpy array, where m is len(review) and n is num_feature\n",
    "    \"\"\"\n",
    "\n",
    "    curr_index = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "       if curr_index%1000 == 0.:\n",
    "           print (\"Vectorizing review %d of %d\" % (curr_index, len(reviews)))\n",
    "   \n",
    "       review_feature_vecs[curr_index] = review_to_vec(review, model, num_features)\n",
    "       curr_index += 1\n",
    "       \n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF vectorization\n",
    "def tfidf_vectorizer(train_list,test_list,train_data,test_data):\n",
    "    for i in range(0, len(train_data.review)):\n",
    "        \n",
    "        # Append raw texts as TFIDF vectorizers take raw texts as inputs\n",
    "        train_list.append(clean_review(train_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print (\"Cleaning training review\", i)\n",
    "\n",
    "    for i in range(0, len(test_data.review)):\n",
    "        \n",
    "        # Append raw texts as TFIDF vectorizers take raw texts as inputs\n",
    "        test_list.append(clean_review(test_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print (\"Cleaning test review\", i)\n",
    "    count_vec = TfidfVectorizer(analyzer=\"word\", max_features=10000, ngram_range=(1,2), sublinear_tf=True)\n",
    "    print (\"Vectorizing input texts\")\n",
    "    train_vec = count_vec.fit_transform(train_list)\n",
    "    test_vec = count_vec.transform(test_list)\n",
    "    return train_vec,test_vec,count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing dimensionality reduction using SelectKBest\n",
    "def dimensionality_reduction(train_vec,test_vec,y_train_data):\n",
    "    print (\"Performing feature selection based on chi2 independence test\")\n",
    "    fselect = SelectKBest(chi2,k=500)\n",
    "    train_vec = fselect.fit_transform(train_vec, y_train_data)\n",
    "    test_vec = fselect.transform(test_vec)\n",
    "    return train_vec,test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes classifier\n",
    "def naive_bayes(train_vec,test_vec,y_train_data):\n",
    "    start = time.time()\n",
    "    nb = MultinomialNB()\n",
    "    cv_score = cross_val_score(nb, train_vec,y_train_data, cv=10)\n",
    "    print(\"TrainingMultinomial Naive Bayes\")\n",
    "    nb = nb.fit(train_vec,y_train_data)\n",
    "    pred_naive_bayes = nb.predict(test_vec)\n",
    "    print (\"CV Score = \", cv_score.mean())\n",
    "    print (\"Total time taken for Multinomial Naive Bayes is \", time.time()-start, \" seconds\")\n",
    "    return pred_naive_bayes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest classifier\n",
    "def random_forest(train_vec,test_vec,y_train_data):\n",
    "    start = time.time()\n",
    "    rfc = RFC(n_estimators = 100,oob_score = True,max_features =\"auto\")\n",
    "    print(\"Training %s\" % (\"Random Forest\"))\n",
    "    rfc = rfc.fit(train_vec,y_train_data)\n",
    "    print(\"OOB Score =\", rfc.oob_score_)\n",
    "    pred_random_forest = rfc.predict(test_vec)\n",
    "    print (\"Total time taken for Random Forest is \", time.time()-start, \" seconds\")\n",
    "    return pred_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVC classifier\n",
    "def linear_svc(train_vec,test_vec,y_train_data): \n",
    "    start = time.time()\n",
    "    svc = svm.LinearSVC()\n",
    "    param = {'C': [1e15,1e13,1e11,1e9,1e7,1e5,1e3,1e1,1e-1,1e-3,1e-5]}\n",
    "    print (\"Training SVC\")\n",
    "    svc = GridSearchCV(svc, param,cv=10)\n",
    "    svc = svc.fit(train_vec, y_train_data)\n",
    "    pred_linear_svc = svc.predict(test_vec)\n",
    "    print (\"Optimized parameters:\", svc.best_estimator_)\n",
    "    print (\"Best CV score:\", svc.best_score_)\n",
    "    print (\"Total time taken for Linear SVC is \", time.time()-start, \" seconds\")\n",
    "#     Below confusion matrix code is commented as it takes a lot of time to run. The plots have been added in the project report.\n",
    "#     print(\"Generating confusion matrix\")\n",
    "#     predictions = cross_val_predict(svc, train_vec, y_train_data)\n",
    "#     skplt.metrics.plot_confusion_matrix(y_train_data, predictions)\n",
    "#     plt.show()\n",
    "    return pred_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "def logistic_regression(train_vec,test_vec,y_train_data):\n",
    "    start = time.time()\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "    cv_score = cross_val_score(clf, train_vec,y_train_data, cv=10)\n",
    "    print(\"Training Logistic Regression\")\n",
    "    clf = clf.fit(train_vec,y_train_data)\n",
    "    pred_logistic= clf.predict(test_vec)\n",
    "    print (\"CV Score = \", cv_score.mean())\n",
    "    print (\"Total time taken for Logistic is \", time.time()-start, \" seconds\")\n",
    "    print(\"Plotting Precision recall curve\")\n",
    "    result = clf.predict_proba(train_vec)\n",
    "    skplt.metrics.plot_precision_recall_curve(y_train_data,result)\n",
    "    plt.show()\n",
    "    return pred_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec vectorization\n",
    "def word2vec(train_data,test_data,train_list,test_list):\n",
    "    model_name = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "    model_type = \"bin\"\n",
    "    num_features = 300\n",
    "    for i in range(0, len(train_data.review)):\n",
    "        train_list.append(clean_review(train_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print(\"Cleaning training review\",i)\n",
    "    for i in range(0, len(test_data.review)):\n",
    "        test_list.append(clean_review(test_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print (\"Cleaning test review\", i)\n",
    "    print (\"Loading the pre-trained model\")\n",
    "    #The below part has been commented as the model was loaded, movie reviews were vectorized and stored in below pkl files, \n",
    "    #as this takes a lot of time to execute. \n",
    "    #We are reading the pkl files to get the final vectorized data\n",
    "    \n",
    "    #model = Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "    print (\"Vectorizing training review\")\n",
    "    #train_vec = gen_review_vecs(train_list, model, num_features)\n",
    "    #print (\"Vectorizing test review\")\n",
    "    #test_vec = gen_review_vecs(test_list, model, num_features)\n",
    "    \n",
    "    #print(\"Writing to DataFrame after vectorizing\")\n",
    "    #df_train = pd.DataFrame(train_vec)\n",
    "    #df_test = pd.DataFrame(test_vec)\n",
    "    #df_train.to_pickle(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\train.pkl\")\n",
    "    #df_test.to_pickle(\"test.pkl\")\n",
    "    \n",
    "    y_train_data = train_data.sentiment\n",
    "    train_df= pd.read_pickle(\"train.pkl\")\n",
    "    test_df = pd.read_pickle(\"test.pkl\")\n",
    "    \n",
    "    #Word2Vec cannot be used with Multinomial Naive Bayes as Naive Bayes does not work with negative values \n",
    "    pred_logistic = logistic_regression(train_df,test_df,y_train_data)\n",
    "    pred_random_forest = random_forest(train_df,test_df,y_train_data)\n",
    "    pred_linear_svc = linear_svc(train_df,test_df,y_train_data)\n",
    "    \n",
    "    output = pd.DataFrame(data = {\"id\": test_data.id,\"review\":test_data.review, \"sentiment\": pred_linear_svc})\n",
    "    output.to_csv(\"word2vec_svc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing a custom movie review\n",
    "def test_custom_review(count_vec,train_vec,y_train_data):\n",
    "    print('\\nTest a custom review message')\n",
    "    print('Enter review to be analysed: ',end=\" \")\n",
    "\n",
    "    test = []\n",
    "    test_list = []\n",
    "    test.append(input())\n",
    "    test_review= pd.DataFrame(data = {\"id\": 1, \"review\": test})\n",
    "    print(\"Cleaning the test review\")\n",
    "    for i in range(0, len(test_review.review)):\n",
    "        test_list.append(clean_review(test_review.review[i]))\n",
    "    print(\"Vectorizing the test review\")\n",
    "    test_review_vec = count_vec.transform(test_list)\n",
    "    print(\"Predicting\")\n",
    "    pred_naive_bayes= naive_bayes(train_vec,test_review_vec,y_train_data)\n",
    "    if(pred_naive_bayes == 1):\n",
    "        print(\"The review is predicted positive\")\n",
    "    else:\n",
    "        print(\"The review is predicted negative\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    word2vec_input = []\n",
    "\n",
    "    pred_naive_bayes = []\n",
    "    pred_logistic = []\n",
    "    pred_random_forest = []\n",
    "    pred_linear_svc = []\n",
    "    train_data = pd.read_csv(\"labeledTrainData.tsv\",header=0, delimiter=\"\\t\", quoting=0)\n",
    "    test_data = pd.read_csv(\"testData.tsv\",header=0, delimiter=\"\\t\", quoting=0)\n",
    "\n",
    "    y_train_data = train_data.sentiment\n",
    "\n",
    "    #Vectorization - TFIDF\n",
    "    print(\"Using TFIDF \")\n",
    "    train_vect,test_vec,count_vec= tfidf_vectorizer(train_list,test_list,train_data,test_data)\n",
    "\n",
    "    #Dimensionality Reduction\n",
    "    train_vec,test_vec = dimensionality_reduction(train_vect,test_vec,y_train_data)\n",
    "    \n",
    "    #Prediction \n",
    "    pred_naive_bayes = naive_bayes(train_vec,test_vec,y_train_data)\n",
    "    pred_random_forest = random_forest(train_vec,test_vec,y_train_data)\n",
    "    pred_linear_svc = linear_svc(train_vec,test_vec,y_train_data)\n",
    "    pred_logistic = logistic_regression(train_vec,test_vec,y_train_data)      \n",
    "\n",
    "    #Writing output of classifier with highest accuracy(Linear SVC)to csv \n",
    "    output = pd.DataFrame(data = {\"id\": test_data.id,\"review\":test_data.review, \"sentiment\": pred_linear_svc})\n",
    "    output.to_csv(\"tfidf_svc.csv\", index=False)\n",
    "\n",
    "    print(\"Using pre-trained word2vec model\")\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    pred_logistic = []\n",
    "    pred_random_forest = []\n",
    "    pred_linear_svc = []\n",
    "\n",
    "    word2vec(train_data,test_data,train_list,test_list)\n",
    "\n",
    "    #Test a custom review using  Multinomial Naive Bayes\n",
    "    test_custom_review(count_vec,train_vect,y_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
