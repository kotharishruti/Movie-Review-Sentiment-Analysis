{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shruti\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n",
      "C:\\Users\\shruti\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\shruti\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\shruti\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn import naive_bayes, svm, preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing movie reviews\n",
    "def clean_review(raw_review):\n",
    "    # Remove HTML markup\n",
    "    text = BeautifulSoup(raw_review, \"lxml\")\n",
    "    \n",
    "    #Removing digits and punctuation\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.get_text())\n",
    "    \n",
    "    #Converting to lowercase\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in text if w not in stops]\n",
    "    \n",
    "    # Return a cleaned string\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a feature vector(word2vec averaging) for each movie review\n",
    "def review_to_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    This function generates a feature vector for the given review.\n",
    "    Input:\n",
    "        words: a list of words extracted from a review\n",
    "        model: trained word2vec model\n",
    "        num_features: dimension of word2vec vectors\n",
    "    Output:\n",
    "        a numpy array representing the review\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vec = np.zeros((num_features), dtype=\"float32\")\n",
    "    word_count = 0\n",
    "    \n",
    "    # index2word_set is a set consisting of all words in the vocabulary\n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            word_count += 1\n",
    "            feature_vec += model[word]\n",
    "\n",
    "    feature_vec /= word_count\n",
    "    return feature_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates vectorized movie reviews\n",
    "def gen_review_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Function which generates a m-by-n numpy array from all reviews,\n",
    "    where m is len(reviews), and n is num_feature\n",
    "    Input:\n",
    "            reviews: a list of lists. \n",
    "                     Inner lists are words from each review.\n",
    "                     Outer lists consist of all reviews\n",
    "            model: trained word2vec model\n",
    "            num_feature: dimension of word2vec vectors\n",
    "    Output: m-by-n numpy array, where m is len(review) and n is num_feature\n",
    "    \"\"\"\n",
    "\n",
    "    curr_index = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "       if curr_index%1000 == 0.:\n",
    "           print (\"Vectorizing review %d of %d\" % (curr_index, len(reviews)))\n",
    "   \n",
    "       review_feature_vecs[curr_index] = review_to_vec(review, model, num_features)\n",
    "       curr_index += 1\n",
    "       \n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF vectorization\n",
    "def tfidf_vectorizer(train_list,test_list,train_data,test_data):\n",
    "    for i in range(0, len(train_data.review)):\n",
    "        \n",
    "        # Append raw texts as TFIDF vectorizers take raw texts as inputs\n",
    "        train_list.append(clean_review(train_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print (\"Cleaning training review\", i)\n",
    "\n",
    "    for i in range(0, len(test_data.review)):\n",
    "        \n",
    "        # Append raw texts as TFIDF vectorizers take raw texts as inputs\n",
    "        test_list.append(clean_review(test_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print (\"Cleaning test review\", i)\n",
    "    count_vec = TfidfVectorizer(analyzer=\"word\", max_features=10000, ngram_range=(1,2), sublinear_tf=True)\n",
    "    print (\"Vectorizing input texts\")\n",
    "    train_vec = count_vec.fit_transform(train_list)\n",
    "    test_vec = count_vec.transform(test_list)\n",
    "    return train_vec,test_vec,count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing dimensionality reduction using SelectKBest\n",
    "def dimensionality_reduction(train_vec,test_vec,y_train_data):\n",
    "    print (\"Performing feature selection based on chi2 independence test\")\n",
    "    fselect = SelectKBest(chi2,k=500)\n",
    "    train_vec = fselect.fit_transform(train_vec, y_train_data)\n",
    "    test_vec = fselect.transform(test_vec)\n",
    "    return train_vec,test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes classifier\n",
    "def naive_bayes(train_vec,test_vec,y_train_data):\n",
    "    nb = MultinomialNB()\n",
    "    cv_score = cross_val_score(nb, train_vec,y_train_data, cv=10)\n",
    "    print(\"Training Naive Bayes\")\n",
    "    nb = nb.fit(train_vec,y_train_data)\n",
    "    pred_naive_bayes = nb.predict(test_vec)\n",
    "    print (\"CV Score = \", cv_score.mean())\n",
    "    predictions = cross_val_predict(nb, train_vec, y_train_data)\n",
    "    skplt.metrics.plot_confusion_matrix(y_train_data, predictions)\n",
    "    plt.show()\n",
    "    #result = nb.predict_proba(train_vec)\n",
    "    skplt.metrics.plot_precision_recall_curve(y_train_data, predictions)\n",
    "    plt.show()\n",
    "    return pred_naive_bayes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest classifier\n",
    "def random_forest(train_vec,test_vec,y_train_data):\n",
    "    rfc = RFC(n_estimators = 100,oob_score = True,max_features =\"auto\")\n",
    "    print(\"Training %s\" % (\"Random Forest\"))\n",
    "    rfc = rfc.fit(train_vec,y_train_data)\n",
    "    print(\"OOB Score =\", rfc.oob_score_)\n",
    "    pred_random_forest = rfc.predict(test_vec)\n",
    "    return pred_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVM classifier\n",
    "def linear_svm(train_vec,test_vec,y_train_data):  \n",
    "    svc = svm.LinearSVC()\n",
    "    param = {'C': [1e15,1e13,1e11,1e9,1e7,1e5,1e3,1e1,1e-1,1e-3,1e-5]}\n",
    "    print (\"Training SVM\")\n",
    "    svc = GridSearchCV(svc, param,cv=10)\n",
    "    svc = svc.fit(train_vec, y_train_data)\n",
    "    pred_linear_svm = svc.predict(test_vec)\n",
    "    print (\"Optimized parameters:\", svc.best_estimator_)\n",
    "    print (\"Best CV score:\", svc.best_score_)\n",
    "    return pred_linear_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Logistic Regression\n",
    "def logistic_regression(train_vec,test_vec,y_train_data):\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "    cv_score = cross_val_score(clf, train_vec,y_train_data, cv=10)\n",
    "    print(\"Training Logistic Regression\")\n",
    "    clf = clf.fit(train_vec,y_train_data)\n",
    "    pred_logistic= clf.predict(test_vec)\n",
    "    print (\"CV Score = \", cv_score.mean())\n",
    "    return pred_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec vectorization\n",
    "def word2vec(train_data,test_data,train_list,test_list):\n",
    "    model_name = \"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "    model_type = \"bin\"\n",
    "    num_features = 300\n",
    "    for i in range(0, len(train_data.review)):\n",
    "        train_list.append(clean_review(train_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print(\"Cleaning training review\",i)\n",
    "    for i in range(0, len(test_data.review)):\n",
    "        test_list.append(clean_review(test_data.review[i]))\n",
    "        if i%1000 == 0:\n",
    "            print (\"Cleaning test review\", i)\n",
    "    print (\"Loading the pre-trained model\")\n",
    "    #The below part has been commented as the model was loaded, movie reviews were vectorized and stored in below pkl files, \n",
    "    #as this takes a lot of time to execute. \n",
    "    #We are reading the pkl files to get the final vectorized data\n",
    "    \n",
    "    #model = Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "    print (\"Vectorizing training review\")\n",
    "    #train_vec = gen_review_vecs(train_list, model, num_features)\n",
    "    #print (\"Vectorizing test review\")\n",
    "    #test_vec = gen_review_vecs(test_list, model, num_features)\n",
    "    \n",
    "    #print(\"Writing to DataFrame after vectorizing\")\n",
    "    #df_train = pd.DataFrame(train_vec)\n",
    "    #df_test = pd.DataFrame(test_vec)\n",
    "    #df_train.to_pickle(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\train.pkl\")\n",
    "    #df_test.to_pickle(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\test.pkl\")\n",
    "    \n",
    "    \n",
    "    y_train_data = train_data.sentiment\n",
    "    train_df= pd.read_pickle(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\train.pkl\")\n",
    "    test_df = pd.read_pickle(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\test.pkl\")\n",
    "    \n",
    "    #Word2Vec cannot be used with Naive Bayes as Naive Bayes does not work with negative values \n",
    "    pred_logistic = logistic_regression(train_df,test_df,y_train_data)\n",
    "    pred_random_forest = random_forest(train_df,test_df,y_train_data)\n",
    "    pred_linear_svm = linear_svm(train_df,test_df,y_train_data)\n",
    "    \n",
    "    output = pd.DataFrame(data = {\"id\": test_data.id,\"review\":test_data.review, \"sentiment\": pred_linear_svm})\n",
    "    output.to_csv(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\word2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing a custom movie review\n",
    "def test_custom_review(count_vec,train_vec,y_train_data):\n",
    "    print('\\nTest a custom review message')\n",
    "    print('Enter review to be analysed: ',end=\" \")\n",
    "\n",
    "    test = []\n",
    "    test_list = []\n",
    "    test.append(input())\n",
    "    test_review= pd.DataFrame(data = {\"id\": 1, \"review\": test})\n",
    "    print(\"Cleaning the test review\")\n",
    "    for i in range(0, len(test_review.review)):\n",
    "        test_list.append(clean_review(test_review.review[i]))\n",
    "    print(\"Vectorizing the test review\")\n",
    "    test_review_vec = count_vec.transform(test_list)\n",
    "    print(\"Predicting\")\n",
    "    #pred_logistic= logistic_regression(train_vec,test_review_vec,y_train_data)\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "    print(\"Training Logistic Regression\")\n",
    "    clf = clf.fit(train_vec,y_train_data)\n",
    "    pred_logistic= clf.predict(test_review_vec)\n",
    "    \n",
    "    if(pred_logistic == 1):\n",
    "        print(\"The review is predicted positive\")\n",
    "    else:\n",
    "        print(\"The review is predicted negative\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TFIDF \n",
      "Cleaning training review 0\n",
      "Cleaning training review 1000\n",
      "Cleaning training review 2000\n",
      "Cleaning training review 3000\n",
      "Cleaning training review 4000\n",
      "Cleaning training review 5000\n",
      "Cleaning training review 6000\n",
      "Cleaning training review 7000\n",
      "Cleaning training review 8000\n",
      "Cleaning training review 9000\n",
      "Cleaning training review 10000\n",
      "Cleaning training review 11000\n",
      "Cleaning training review 12000\n",
      "Cleaning training review 13000\n",
      "Cleaning training review 14000\n",
      "Cleaning training review 15000\n",
      "Cleaning training review 16000\n",
      "Cleaning training review 17000\n",
      "Cleaning training review 18000\n",
      "Cleaning training review 19000\n",
      "Cleaning training review 20000\n",
      "Cleaning training review 21000\n",
      "Cleaning training review 22000\n",
      "Cleaning training review 23000\n",
      "Cleaning training review 24000\n",
      "Cleaning test review 0\n",
      "Cleaning test review 1000\n",
      "Cleaning test review 2000\n",
      "Cleaning test review 3000\n",
      "Cleaning test review 4000\n",
      "Cleaning test review 5000\n",
      "Cleaning test review 6000\n",
      "Cleaning test review 7000\n",
      "Cleaning test review 8000\n",
      "Cleaning test review 9000\n",
      "Cleaning test review 10000\n",
      "Cleaning test review 11000\n",
      "Cleaning test review 12000\n",
      "Cleaning test review 13000\n",
      "Cleaning test review 14000\n",
      "Cleaning test review 15000\n",
      "Cleaning test review 16000\n",
      "Cleaning test review 17000\n",
      "Cleaning test review 18000\n",
      "Cleaning test review 19000\n",
      "Cleaning test review 20000\n",
      "Cleaning test review 21000\n",
      "Cleaning test review 22000\n",
      "Cleaning test review 23000\n",
      "Cleaning test review 24000\n",
      "Vectorizing input texts\n",
      "Performing feature selection based on chi2 independence test\n",
      "Training Naive Bayes\n",
      "CV Score =  0.8665199999999998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shruti\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function plot_precision_recall_curve is deprecated; This will be removed in v0.5.0. Please use scikitplot.metrics.plot_precision_recall instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c7c1987939a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#Prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpred_naive_bayes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m#pred_random_forest = random_forest(train_vec,test_vec,y_train_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#pred_linear_svm = linear_svm(train_vec,test_vec,y_train_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-24208f904a44>\u001b[0m in \u001b[0;36mnaive_bayes\u001b[1;34m(train_vec, test_vec, y_train_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#result = nb.predict_proba(train_vec)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mskplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_precision_recall_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpred_naive_bayes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scikitplot\\metrics.py\u001b[0m in \u001b[0;36mplot_precision_recall_curve\u001b[1;34m(y_true, y_probas, title, curves, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         precision[i], recall[i], _ = precision_recall_curve(\n\u001b[1;32m--> 625\u001b[1;33m             y_true, probas[:, i], pos_label=classes[i])\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    word2vec_input = []\n",
    "    \n",
    "    pred_naive_bayes = []\n",
    "    pred_logistic = []\n",
    "    pred_random_forest = []\n",
    "    pred_linear_svm = []\n",
    "    train_data = pd.read_csv(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\labeledTrainData.tsv\",header=0, delimiter=\"\\t\", quoting=0)\n",
    "    test_data = pd.read_csv(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\testData.tsv\",header=0, delimiter=\"\\t\", quoting=0)\n",
    "    \n",
    "    y_train_data = train_data.sentiment\n",
    "    \n",
    "    #Vectorization - TFIDF\n",
    "    print(\"Using TFIDF \")\n",
    "    train_vect,test_vec,count_vec= tfidf_vectorizer(train_list,test_list,train_data,test_data)\n",
    "    \n",
    "    #Dimensionality Reduction\n",
    "    train_vec,test_vec = dimensionality_reduction(train_vect,test_vec,y_train_data)\n",
    "    \n",
    "    #Prediction \n",
    "    pred_naive_bayes = naive_bayes(train_vec,test_vec,y_train_data)\n",
    "    #pred_random_forest = random_forest(train_vec,test_vec,y_train_data)\n",
    "    #pred_linear_svm = linear_svm(train_vec,test_vec,y_train_data)\n",
    "    #pred_logistic = logistic_regression(train_vec,test_vec,y_train_data)      \n",
    "    \n",
    "    #output = pd.DataFrame(data = {\"id\": test_data.id,\"review\":test_data.review, \"sentiment\": pred_linear_svm})\n",
    "    #output.to_csv(\"C:\\\\Users\\\\shruti\\\\sentiment.analysis\\\\tfidf_svm.csv\", index=False)\n",
    "    \n",
    "    print(\"Using pre-trained word2vec model\")\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    pred_logistic = []\n",
    "    pred_random_forest = []\n",
    "    pred_linear_svm = []\n",
    "    \n",
    "    word2vec(train_data,test_data,train_list,test_list)\n",
    "    \n",
    "    #Test a custom review using Logistic Regression\n",
    "    test_custom_review(count_vec,train_vect,y_train_data)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
